# 学習資料 - 学習に関するテクニック - 
## 概要
学習に関するテクニックを学習します。

## 目標
- [ ] 00, 勾配消失問題

最適化手法：パラメータの更新<br>
- [ ] 01, SGD
- [ ] 02, Momentum
- [ ] 03, AdaGrad
- [ ] 04, RMSProp
- [ ] 05, Adam

重みの初期化<br>
- [ ] 06, Xavier
- [ ] 07, He

バッチ正規化<br>
- [ ] 08, バッチ正規化 

過学習を防ぐ方法<br>
- [ ] 09, 過学習とは
- [ ] 10, Weight decay
- [ ] 11, Dropout
- [ ] 12, early stopping
- [ ] 13, 正則化（学習済み）

ハイパーパラメータの最適化<br>
- [ ] 12, ベイズ最適化

オートエンコーダ<br>
- [ ] 15, オートエンコーダ
- [ ] 16, 積層オートエコーダ

## 学習方法
以下の参考資料を使って目標を達成します。

## 学習項目
- [【深層学習】勾配消失問題とは？ニューラルネットワーク学習時の対処方法｜ディープラーニング入門](https://di-acc2.com/analytics/ai/6315/)

最適化手法<br>
- [【ディープラーニング】損失関数とパラメータ探索アルゴリズム｜勾配降下法・学習率・局所最適解と大域最適解問題も徹底解説](https://di-acc2.com/analytics/ai/6296/)
- [SGD【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/09/180000)
- [Momentum【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/10/180000)
- [AdaGrad【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/11/180000)
- [RMSProp【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/12/180000)
- [Adam【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/13/180000)

重みの初期化<br>
- [重みの初期値【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/15/180000)

バッチ正規化<br>
- [Batch Normalization【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/16/180000)

過学習を防ぐ方法<br>
- [Weight decay【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/18/180000)
- [Dropout【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/19/180000)
- [deep learningの基礎（Early Stopping)](https://note.com/shantiboy/n/n32360fe14fd4)

ハイパーパラメータの最適化<br>
- [ハイパーパラメータの検証【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/20/180000)
- [機械学習のためのベイズ最適化入門](https://book.mynavi.jp/manatee/detail/id=59393)

オートエンコーダ<br>
- [AE(オートエンコーダ)とは?図解で分かりやすく解説!!](https://nisshingeppo.com/ai/whats-autoencorder/)
- [オートエンコーダ（自己符号化器）とは](https://jp.mathworks.com/discovery/autoencoder.html)
