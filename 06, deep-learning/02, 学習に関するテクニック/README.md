# 学習に関するテクニック
- 勾配消失問題

最適化手法：パラメータの更新<br>
- SGD
- Momentum
- AdaGrad
- RMSProp
- Adam

重みの初期化<br>
- Xavier
- He

バッチ正規化<br>
- バッチ正規化 

過学習を防ぐ方法<br>
- 過学習とは
- Weight decay
- Dropout
- early stopping
- 正則化（学習済み）

ハイパーパラメータの最適化<br>
- ベイズ最適化

オートエンコーダ<br>
- オートエンコーダ
- 積層オートエコーダ

## web
- [【深層学習】勾配消失問題とは？ニューラルネットワーク学習時の対処方法｜ディープラーニング入門](https://di-acc2.com/analytics/ai/6315/)

最適化手法<br>
- [【ディープラーニング】損失関数とパラメータ探索アルゴリズム｜勾配降下法・学習率・局所最適解と大域最適解問題も徹底解説](https://di-acc2.com/analytics/ai/6296/)
- [SGD【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/09/180000)
- [Momentum【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/10/180000)
- [AdaGrad【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/11/180000)
- [RMSProp【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/12/180000)
- [Adam【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/13/180000)

重みの初期化<br>
- [重みの初期値【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/15/180000)

バッチ正規化<br>
- [Batch Normalization【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/16/180000)

過学習を防ぐ方法<br>
- [Weight decay【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/18/180000)
- [Dropout【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/19/180000)
- [deep learningの基礎（Early Stopping)](https://note.com/shantiboy/n/n32360fe14fd4)

ハイパーパラメータの最適化<br>
- [ハイパーパラメータの検証【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/20/180000)
- [機械学習のためのベイズ最適化入門](https://book.mynavi.jp/manatee/detail/id=59393)

オートエンコーダ<br>
- [AE(オートエンコーダ)とは?図解で分かりやすく解説!!](https://nisshingeppo.com/ai/whats-autoencorder/)
- [オートエンコーダ（自己符号化器）とは](https://jp.mathworks.com/discovery/autoencoder.html)
