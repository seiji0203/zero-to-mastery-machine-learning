# 学習資料 - ニューラルネットワーク - 
## 概要
ニューラルネットワークを学習します。

## 目標
- [ ] 00, 単純パーセプトロン
- [ ] 01, 多層パーセプトロン
- [ ] 02, ADALINE

活性化関数<br>
- [ ] 03, ステップ関数
- [ ] 04, 恒等関数
- [ ] 05, シグモイド関数
- [ ] 06, tanh関数
- [ ] 07 ReLU関数
- [ ] 08, leaky-ReLU関数
- [ ] 09, ソフトマックス関数
- [ ] 10, GELU関数

損失関数<br>
- [ ] 11, 2乗和誤差
- [ ] 12, 交差エントロピー誤差
- [ ] 13, ミニバッチ学習

数値微分<br>
- [ ] 14, 偏微分
- [ ] 15, 勾配降下法
- [ ] 16, バッチ勾配降下法（最急降下法）
- [ ] 17, 確率的勾配降下法（SGD）

誤差逆伝播法<br>
- [ ] 16, 誤差逆伝播法
- [ ] 17, 連鎖律
- [ ] 18, 順伝播
- [ ] 19, 逆伝播


## 学習方法
以下の参考資料を使って目標を達成します。

## 学習項目
- [ ] 00, [【徹底解説】ニューラルネットワークって一体なに？【人工知能】](https://rightcode.co.jp/blog/information-technology/neuralnetwork)
- [ ] 01, [機械学習の元祖「パーセプトロン」とは？](https://rightcode.co.jp/blog/information-technology/simple-perceptron)
- [ ] 02, [ニューラルネットワークの基本であるパーセプトロンの性能限界とは？](https://rightcode.co.jp/blog/information-technology/neural-network-perceptron-performance-limit)
- [ ] 03, [多層パーセプトロンを実装してみよう！【機械学習】](https://rightcode.co.jp/blog/information-technology/multilayer-perceptron-implementation)
- [ ] 04, [ニューラルネットワークの学習過程を可視化してみよう](https://rightcode.co.jp/blog/information-technology/neural-network-learning-visualization)
- [ ] 05, [ニューラルネットワークの基礎 - Chainer](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html)
- [ ] 06, [【機械学習】深層学習(ディープラーニング)とは何か](https://www.youtube.com/watch?v=s5_Pk3CjhNA)
- [ ] 07, [パーセプトロン【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/02/180000)
- [ ] 08, [パーセプトロンからニューラルネットワークへ【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/03/180000)
- [ ] 09, [ニューラルネットワークの順伝播【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/06/180000)

活性化関数<br>
- [ ] 10, [活性化関数一覧 (2020)](https://qiita.com/kuroitu/items/73cd401afd463a78115a)

損失関数<br>
- [ ] 11, [損失関数とは？ニューラルネットワークの学習理論【機械学習】](https://rightcode.co.jp/blog/information-technology/loss-function-neural-network-learning-theory)
- [ ] 12, [2乗和誤差の実装【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/16/180000)
- [ ] 13, [交差エントロピー誤差の実装【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/17/180000)
- [ ] 14, [ミニバッチ学習【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/22/180000)

数値微分<br>
- [ ] 15, [数値微分【ゼロつく1のノート(数学)】](https://www.anarchive-beta.com/entry/2020/06/18/180000)
- [ ] 16, [勾配【ゼロつく1のノート(数学)】](https://www.anarchive-beta.com/entry/2020/06/19/180000)
- [ ] 17, [ニューラルネットワークに対する勾配【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/06/20/180000)
- [ ] 18, [連鎖率【ゼロつく1のノート(数学)】](https://www.anarchive-beta.com/entry/2020/07/29/180000)

誤差逆伝播法<br>
- [ ] 19, [誤差逆伝播法に対応したニューラルネットワークの実装【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/07/180000)
- [ ] 20, [誤差逆伝播法を使った学習【ゼロつく1のノート(実装)】](https://www.anarchive-beta.com/entry/2020/08/08/180000)
- [ ] 21, [機械学習の要「誤差逆伝播学習法」を解説・実装してみる！](https://rightcode.co.jp/blog/information-technology/back-propagation-algorithm-implementation)
- [ ] 22, [絶対に理解させる誤差逆伝播法【深層学習】](https://www.youtube.com/watch?v=0itH0iDO8BE)
