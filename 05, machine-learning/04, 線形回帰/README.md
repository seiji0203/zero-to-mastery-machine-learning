# 線形回帰
## 目標
- 単回帰分析
- 重回帰分析
- 最小二乗法
- Gauss型基底関数
- 正則化（L1：Lasso回帰）
- 正則化（L2：Ridge回帰）
- ElasticNet
- RANSAC
- ポアソン回帰

## web
- [機械学習とは？なにをしているのか？【機械学習入門1】](https://datawokagaku.com/what_is_machine_learning/)
- [線形回帰の損失関数をわかりやすく解説【機械学習入門2】](https://datawokagaku.com/cost_function/)
- [最急降下法を図と数式で理解する(超重要)【機械学習入門3】](https://datawokagaku.com/gradient_descent/)
- [正規方程式を完全解説(導出あり)【機械学習入門4】](https://datawokagaku.com/normal_equation/)
- [scikit-learnを使って線形回帰モデルを構築する【機械学習入門5】](https://datawokagaku.com/linear_reg_implement/)
- [線形回帰の係数の解釈の仕方(p値)【機械学習入門6】](https://datawokagaku.com/lin_reg_coef/)
- [多項式特徴量で線形を超える！【機械学習入門11】](https://datawokagaku.com/polynomial_features/)
- [正則化項を用いて特徴量選択をする(Lasso)【機械学習入門14】](https://datawokagaku.com/lasso/)

- [単回帰分析と重回帰分析](https://tutorials.chainer.org/ja/07_Regression_Analysis.html)
- [【機械学習】入門⑤　線形基底関数モデル　Pythonで学ぶ「教師あり学習」　回帰編](https://www.wantanblog.com/entry/2020/02/19/220917)
- [【機械学習】過学習を防ぐ「正則化」](https://rightcode.co.jp/blog/information-technology/regularization-to-prevent-overtraining)
- [超入門！リッジ回帰・Lasso回帰・Elastic Netの基本と特徴をサクッと理解！](https://aizine.ai/ridge-lasso-elasticnet/)
- [L1/L2正則化の意味【機械学習】](https://www.youtube.com/watch?v=3vfiMRjgzZ8)
- [ElasticNetの概要とパラメータチューニングについて](https://qiita.com/c60evaporator/items/d0356fca12b37a82fe57)
- [【Python】RANSACを用いてロバスト回帰モデルを実装｜scikit-learn・機械学習による線形回帰分析](https://di-acc2.com/programming/python/12459/)
- [【統計モデリング】scikit-learnが一般化線形モデル（GLM）をサポートしたので使ってみた](https://cpp-learning.com/scikit-learn-glm/)
